## understanding SVM
直观理解SVM。

## 原理
## 线性可分数据
考虑线面两类数据，红色和蓝色。在kNN中，测试数据，用测量所有数据和新入数据之间的距离。kNN会占用很多内存来存储样本，很长时间类测试所有的距离。但是对于下图。我们真的需要计算那么多吗?

<image src="image/02-01.png"/>

另一种思路：找一条线，`f(x)=ax1+bx2+c`将数据分为两个区域。当我们获得新的数据X，用f(x)进行替代。如果`f(X)>0`,将属于蓝色组，其他属于红色组。称这个为Decision Boundary。很简单，并且内存效率。这些数据，可被分为两个区域，成为Linear Separable.

所以在上面图片中，你能看到有很多种可能的图片。要哪一个？直觉需要那条能穿过所有点的线。为什么？因为输入的数据会产生噪音。数据不应该影响划分的精度。SVM做的方法是找一条直线在划分样本中的最短距离。看加粗直线通过中心区域的：

<image src="image/02-02.png"/>
我们找到决定边界，需要训练数据。需要所有的吗？并不要，只要接近相对立的组就足够了。在我们图片中，有蓝色调控圆，和两个红色填充方块。我们称为Support Vectors并且线穿过他们称为Support Planes.他们足够我们找到边界线。我们不用担心所有的数据，帮我们减少数据。

现在是，很好代表数据的前两个超平面被找到了。例如：蓝色数据用`wTx+b0>1`，而红色的用`wTx+b0<-1`这里，w是weight vector(`w=[w1,w2,..,wn]`)并且x是特征向量(x=[x1,x2,...,xn])。b0是偏移bias。比重向量决定决定边界的方向，而偏移点决定位置。现在决定边界定义在超平面中间位置，便是为`wT+b0=0`.support vector到decision boudary最短距离为<image src="image/02-03.png"/>，页边空白是这个距离的两倍，所以需要最大化空白。例如：需要最小化函数`L(w,b0)`其中这些约束表示为：

<image src="image/02-04.png"/>

ti每个累的label。ti属于[-1,1]

## No-Linearly Separable Data
考虑一些不能划分为直线的。如，一维数据集，`X`属于`-3,3`，`O`属于`-1,1`。很明显不是线性划分。如果我们使用函数`f(x)=x^2`,将会得到`X`在9，而`O`是1,这个就可以线性划分。

否则我们将一维变成二维数据。使用`f(x)=(x,x^2)`来映射这些数据。那么`X`变成`(-3,9)`和`(3,9)`而`O`变成`(-1,1)`和`(1,1)`.这也是线性划分。简单说。低维非线性数据，很容易变为高维线性数据。

一般情况，很容易将d维空间点映射到D维空间(D>d),来检测可能线性分割。这个思想帮我们低位输入高维来划分。用下面例子来说明：

考虑二维空间点。p=(p1,p2)和q=(q1,q2)。让Ø作为二维到三维空间的映射。

<image src="image/02-05.png"/>

定义一个内核函数K(p,q)，两个点之间点乘。结果如下：

<image src="image/02-06.png"/>

这个表示，一个三维空间的点乘，可以使用二维空间点乘平方来获得。这个可以对高维使用。所以可以利用从低维空间特性来计算高维，一旦我们映射好，就可以得到高维特性。

加入这些概念外，将会错分类的问题。所以仅仅找到决定边界最大空白不不够的。我们需要考虑错分类的错误问题。有时候，有可能找到决定边界有少的空白，但能减少分类错误。无论如何，我们想找到大空白，少分类错误。最小条件改为：

<image src="image/02-07.png"/>

下图显示这个概念。每个训练集的数据一个新的参数ξi被定义。这个是训练集合他们正确值之间的距离。对于那些，没有分错的，应该位于support  plane上，他们距离是0.

<image src="image/02-08.png"/>
所以最优化问题是：

<image src="image/02-09.png"/>

参数C选择的问题？很明显回答这个问题，依赖于训练数据如何分布的。虽然不是一般的回答，需要考虑这些规则：

* C越大获得错误分类越少，但是空白越小。这个情况下，减少分类错误的代价是很大的。由于目标是最小化参数，少部分分类错误是可以接受的。
* C越小，错误越小但能得到更大的空白。这个时候最小化没有考虑太多，而在于找打超平面有大的空白。